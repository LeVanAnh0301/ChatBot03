{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install tflearn \n"
      ],
      "metadata": {
        "id": "7a7M-s36008N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c543383-2ccb-476a-cc7e-21f89ea3e8f2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tflearn in /usr/local/lib/python3.7/dist-packages (0.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tflearn) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5ijryB-x6xis"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zpllH5CUypBB"
      },
      "outputs": [],
      "source": [
        "#Used in Tensorflow Model\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tflearn\n",
        "import random\n",
        "\n",
        "#Used NLTK ( a library in NLP)\n",
        "import nltk\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "#Other\n",
        "import json\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading data training.\n",
        "with open('intents.json') as json_data:\n",
        "    intents = json.load(json_data)"
      ],
      "metadata": {
        "id": "S3O0GGJs0Ug-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trong file json \n",
        "\n",
        "+ tag (nhãn lớp cho nội dung nhập của người dùng)\n",
        "+ patterns - mẫu câu đầu vào được training để phân lớp\n",
        "+ responses - các câu trả lời (bot) được mapping để hồi đáp những request trước đó."
      ],
      "metadata": {
        "id": "cK92xD7V1Bbt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Với 'intents.json' vừa được đưa vào bộ training, ta cần tổ chức lại nó, Xác định cho công cụ của bạn biết đâu là documents dùng để training, các từ, các classes để phân lớp. Nên sử dụng thêm bộ công cụ của xử lý ngôn ngữ tự nhiên nltk để tiền xử lý dữ liệu. Bộ công cụ này cho phép bạn thực hiện các quá trình tokenizer, POS stagging, word segmentation, remove stopword...."
      ],
      "metadata": {
        "id": "aUHtJ82cHWXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('punkt')\n",
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?']\n",
        "print(\"Looping through the Intents to Convert them to words, classes, documents and ignore_words.......\")\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        # tokenize each word in the sentence\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        # add to our words list\n",
        "        words.extend(w)\n",
        "        # add to documents in our corpus\n",
        "        documents.append((w, intent['tag']))\n",
        "        # add to our classes list\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrUyGGT908Hg",
        "outputId": "155f7a8c-ced5-4d65-fde3-d0620c2a6fd7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looping through the Intents to Convert them to words, classes, documents and ignore_words.......\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Stemming, Lowering and Removing Duplicates.......\")\n",
        "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "# remove duplicates\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "print (len(documents), \"documents\")\n",
        "print (len(classes), \"classes\", classes)\n",
        "print (len(words), \"unique stemmed words\", words)"
      ],
      "metadata": {
        "id": "B0CxJaI41UXK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "366e37a6-0015-42fd-d99c-3ff856ee2e15"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming, Lowering and Removing Duplicates.......\n",
            "27 documents\n",
            "9 classes ['goodbye', 'greeting', 'hours', 'mopeds', 'opentoday', 'payments', 'rental', 'thanks', 'today']\n",
            "48 unique stemmed words [\"'d\", \"'s\", 'a', 'acceiv', 'anyon', 'ar', 'bye', 'can', 'card', 'cash', 'credit', 'day', 'do', 'doe', 'good', 'goodby', 'hav', 'hello', 'help', 'hi', 'hour', 'how', 'i', 'is', 'kind', 'lat', 'lik', 'mastercard', 'mop', 'of', 'on', 'op', 'rent', 'see', 'tak', 'thank', 'that', 'ther', 'thi', 'to', 'today', 'we', 'what', 'when', 'which', 'work', 'yo', 'you']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**stopwords**  đại khái thì nó là danh sách các từ xuất hiện nhiều trong văn bản nhưng lại không có giá trị trong việc phân lớp. Vì vậy trước khi training, chúng ta cần làm sạch văn bản, loại bỏ những từ ngữ không có ý nghĩa này để tránh overfiting ảnh hưởng đến kết quả training."
      ],
      "metadata": {
        "id": "lYVXhmDRHnNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create our training data\n",
        "training = []\n",
        "output = []\n",
        "\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "# training set, bag of words for each sentence\n",
        "for doc in documents:\n",
        "    # initialize our bag of words\n",
        "    bag = []\n",
        "    # list of tokenized words for the pattern\n",
        "    pattern_words = doc[0]\n",
        "    # stem each word\n",
        "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
        "    # create our bag of words array\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "    # output is a '0' for each tag and '1' for current tag\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "    training.append([bag, output_row])\n",
        "    random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "# create train and test lists\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])"
      ],
      "metadata": {
        "id": "cr_FUvQT1W65"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf  \n",
        "tf.compat.v1.reset_default_graph()\n",
        "print(\"Shuffling Randomly and Converting into Numpy Array for Faster Processing......\")\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "print(\"Creating Train and Test Lists.....\")\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])\n",
        "print(\"Building Neural Network for Out Chatbot to be Contextual....\")\n",
        "print(\"Resetting graph data....\")\n"
      ],
      "metadata": {
        "id": "uKkw8TYN1ZvH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9a14b99-e19d-4894-fc81-bcd298608e59"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shuffling Randomly and Converting into Numpy Array for Faster Processing......\n",
            "Creating Train and Test Lists.....\n",
            "Building Neural Network for Out Chatbot to be Contextual....\n",
            "Resetting graph data....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
        "net = tflearn.regression(net)\n",
        "print(\"Training....\")"
      ],
      "metadata": {
        "id": "d3jzRwn01cIc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b79f973b-2f7e-4c80-9388-941e6ae32f9b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model and setup tensorboard\n",
        "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')"
      ],
      "metadata": {
        "id": "MWpcgDYy1c1K"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "print(\"Training the Model.......\")\n",
        "model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)\n",
        "print(\"Saving the Model.......\")\n",
        "model.save('model.tflearn')"
      ],
      "metadata": {
        "id": "xmPqGTH81kTe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7773256f-c59e-4cc6-c1e4-a605cb5834e4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Step: 7999  | total loss: \u001b[1m\u001b[32m0.05822\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 2000 | loss: 0.05822 - acc: 0.9999 -- iter: 24/27\n",
            "Training Step: 8000  | total loss: \u001b[1m\u001b[32m0.05583\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 2000 | loss: 0.05583 - acc: 0.9999 -- iter: 27/27\n",
            "--\n",
            "Saving the Model.......\n",
            "INFO:tensorflow:/content/model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:/content/model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing data_preprocessing.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'NoneType' object has no attribute 'name'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing data_preprocessing.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'NoneType' object has no attribute 'name'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing data_augmentation.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'NoneType' object has no attribute 'name'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing data_augmentation.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'NoneType' object has no attribute 'name'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing summary_tags.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'dict' object has no attribute 'name'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing summary_tags.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'dict' object has no attribute 'name'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Với cấu trúc như trên, chúng ta đang xây dựng mạng neural 2 lớp để traning.Để hoàn thành quá trình này.\n",
        "\n",
        "bạn cần lưu lại (pickle) model và document để có thể sử dụng lại nó trong quá trình predict ở bước tiếp theo."
      ],
      "metadata": {
        "id": "INdHazWvIn6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Pickle is also Saved..........\")\n",
        "pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( \"training_data\", \"wb\" ) )"
      ],
      "metadata": {
        "id": "zFdl_v3T1nUc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d312437-726a-4cc5-857f-c8f509e62f73"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pickle is also Saved..........\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading Pickle.....\")\n",
        "data = pickle.load( open( \"training_data\", \"rb\" ) )\n",
        "words = data['words']\n",
        "classes = data['classes']\n",
        "train_x = data['train_x']\n",
        "train_y = data['train_y']\n",
        "\n",
        "\n",
        "with open('intents.json') as json_data:\n",
        "    intents = json.load(json_data)\n",
        "    \n",
        "print(\"Loading the Model......\")\n",
        "# load our saved model\n",
        "model.load('./model.tflearn')"
      ],
      "metadata": {
        "id": "ay7ssjrZ1p4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5e86596-7deb-45f2-b9fa-f11ea58e64e8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Pickle.....\n",
            "Loading the Model......\n",
            "INFO:tensorflow:Restoring parameters from /content/model.tflearn\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/model.tflearn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "với các ý định người dùng nhập vào bạn cũng cần phải tiền xử lý, tokenizer, hay chuyển sang bag-of-words để hệ thống có thể hiểu và phân loại về đúng lớp của nó"
      ],
      "metadata": {
        "id": "wW7ULLe6E9NK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_up_sentence(sentence):\n",
        "    # It Tokenize or Break it into the constituents parts of Sentense.\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    # Stemming means to find the root of the word.\n",
        "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n",
        "# Return the Array of Bag of Words: True or False and 0 or 1 for each word of bag that exists in the Sentence\n",
        "def bow(sentence, words, show_details=False):\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    bag = [0]*len(words)\n",
        "    for s in sentence_words:\n",
        "        for i,w in enumerate(words):\n",
        "            if w == s:\n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print (\"found in bag: %s\" % w)\n",
        "    return(np.array(bag))\n",
        "#bộ xử lý phản hồi của chatbot\n",
        "\n",
        "ERROR_THRESHOLD = 0.25\n",
        "print(\"ERROR_THRESHOLD = 0.25\")\n",
        "\n",
        "def classify(sentence):\n",
        "    # Prediction or To Get the Posibility or Probability from the Model\n",
        "    results = model.predict([bow(sentence, words)])[0]\n",
        "    # Exclude those results which are Below Threshold\n",
        "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
        "    # Sorting is Done because heigher Confidence Answer comes first.\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append((classes[r[0]], r[1])) #Tuppl -> Intent and Probability\n",
        "    return return_list\n",
        "\n",
        "def response(sentence, userID='123', show_details=False):\n",
        "    results = classify(sentence)\n",
        "    # That Means if Classification is Done then Find the Matching Tag.\n",
        "    if results:\n",
        "        # Long Loop to get the Result.\n",
        "        while results:\n",
        "            for i in intents['intents']:\n",
        "                # Tag Finding\n",
        "                if i['tag'] == results[0][0]:\n",
        "                    # Random Response from High Order Probabilities\n",
        "                    return print(random.choice(i['responses']))\n",
        "\n",
        "            results.pop(0)"
      ],
      "metadata": {
        "id": "0Y4fDgNq1tbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf9809d3-1821-46e1-a988-2adebd5e296a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR_THRESHOLD = 0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    input_data = input(\"You- \")\n",
        "    answer = response(input_data)\n",
        "    answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IWEGB5vJrl9",
        "outputId": "c3912b44-bc23-4241-e21e-d4c53713c285"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You- Hi\n"
          ]
        }
      ]
    }
  ]
}